#!/usr/bin/env python3
"""
Streamlit RAG ì•± â€” Drone Info Assistant
- FAISS ì¸ë±ìŠ¤ì—ì„œ ë¬¸ë‹¨ì„ ê²€ìƒ‰í•˜ê³  LLMìœ¼ë¡œ ê·¼ê±° í¬í•¨ ë‹µë³€ ìƒì„±
- Ollama (ë¡œì»¬) ë˜ëŠ” OpenAI ì‚¬ìš© ê°€ëŠ¥
"""

import os
from pathlib import Path
import re
import glob
import faiss
import streamlit as st
from typing import List, Dict, Optional
from langdetect import detect, LangDetectException

from langchain_core.documents import Document
from langchain_community.document_loaders import PyPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.embeddings import OllamaEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_community.chat_models import ChatOllama
from langchain_community.docstore import InMemoryDocstore

# -------- Config --------
PDF_DIR = "drone_pdfs"
INDEX_DIR = "vector_index"
MODEL_NAME = "llama3.1:8b"
USE_OLLAMA_DEFAULT = True 
CHUNK_SIZE = 1000
CHUNK_OVERLAP = 200

def classify_category(filename: str) -> str:
    """íŒŒì¼ëª… ê¸°ë°˜ ê°„ë‹¨í•œ ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜"""
    name = filename.lower()
    if "law" in name or "ë²•" in name:
        return "ë²•ë¥ "
    elif "manual" in name or "guide" in name or "controller" in name:
        return "ë§¤ë‰´ì–¼"
    else:
        return "ê¸°íƒ€"
    
def clean_text(text: str) -> str:
    text = re.sub(r'\r\n', '\n', text)
    text = re.sub(r'\s+', ' ', text)
    text = re.sub(r'-{2,}', ' ', text)
    text = re.sub(r'Page\s*\d+(/\d+)?', '', text, flags=re.IGNORECASE)
    text = re.sub(r'[â€¢â—â—†â–¶â†’â€»â– â–¡â–£â—ˆâ—‡â˜…â˜†]', ' ', text)
    return text.strip()

def detect_language(text: str) -> str:
    try:
        lang = detect(text)
        if lang.startswith("ko"): return "ko"
        if lang.startswith("en"): return "en"
        return "other"
    except LangDetectException:
        return "unknown"

def load_all_data(pdf_dir: str, txt_dir: str = 'data') -> List[Document]:
    """txt_dir: for MD, RST"""
    pdf_paths = glob.glob(os.path.join(pdf_dir, "*.pdf"))

    docs: List[Document] = []
    for p in sorted(pdf_paths):
        try:
            loader = PyPDFLoader(p)
            loaded = loader.load()
            category = classify_category(os.path.basename(p))
            for d in loaded:
                d.page_content = clean_text(d.page_content)
                d.metadata["source"] = os.path.basename(p)
                d.metadata["category"] = category 
            docs.extend(loaded)
        except Exception as e:
            st.warning(f"PDF ë¡œë“œ ì‹¤íŒ¨: {os.path.basename(p)} -> {e}")

    for path in Path(txt_dir).rglob('*'):
        if path.suffix.lower() not in ['.md', '.rst']:
            continue
        with open(path, 'r', encoding='utf-8') as f:
            content = f.read()

        docs.append(Document(
            page_content=content,
            metadata = {
                'source': os.path.basename(path),
                'category': 'ë§¤ë‰´ì–¼',
            }
        ))
        
    return docs

def build_vectorstore(pdf_dir: str = PDF_DIR, index_dir: str = INDEX_DIR) -> Optional[FAISS]:
    docs = load_all_data(pdf_dir)
    if not docs:
        st.error("ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. drone_pdfs/data í´ë”ë¥¼ í™•ì¸í•˜ì„¸ìš”.")
        return None

    splitter = RecursiveCharacterTextSplitter(chunk_size=CHUNK_SIZE, chunk_overlap=CHUNK_OVERLAP, add_start_index=True)
    splits = splitter.split_documents(docs)
    embeddings = OllamaEmbeddings(model=MODEL_NAME)
    emb_dim = len(embeddings.embed_query("test"))
    index = faiss.IndexFlatL2(emb_dim)
    vectorstore = FAISS(embedding_function=embeddings, index=index, docstore=InMemoryDocstore(), index_to_docstore_id={})
    for i in range(0, len(splits), 100):
        vectorstore.add_documents(splits[i:i+100])
    os.makedirs(index_dir, exist_ok=True)
    vectorstore.save_local(index_dir)
    return vectorstore

@st.cache_resource(show_spinner=False)
def load_or_build_index(index_dir: str = INDEX_DIR):
    embeddings = OllamaEmbeddings(model=MODEL_NAME)
    if os.path.exists(index_dir):
        try:
            return FAISS.load_local(index_dir, embeddings, allow_dangerous_deserialization=True)
        except Exception:
            pass
    return build_vectorstore()

def get_llm_stream(stream_mode=True):
    return ChatOllama(model=MODEL_NAME, temperature=0.3, stream=stream_mode)

def get_embeddings(use_ollama: bool):
    return OllamaEmbeddings(model=MODEL_NAME)

def docs_to_display_text(docs: List[Document]) -> str:
    """ê²€ìƒ‰ëœ ë¬¸ì„œ(ë“¤)ë¥¼ ì½ê¸° ì¢‹ì€ ë¬¸ìì—´ë¡œ ë³€í™˜ (ì¶œì²˜ í¬í•¨)"""
    parts = []
    for d in docs:
        meta = d.metadata or {}
        title = meta.get("title", meta.get("source", "unknown"))
        chunk_id = meta.get("chunk_id", None)
        src = f"{title}" + (f" (chunk {chunk_id})" if chunk_id is not None else "")
        text = d.page_content if hasattr(d, "page_content") else str(d)
        parts.append(f"---\nğŸ“˜ {src}\n{text}")
    return "\n\n".join(parts)

# -------- Streamlit UI --------
st.set_page_config(page_title="ğŸ›¸ Drone Info Assistant (RAG)", layout="wide", page_icon="ğŸ›¸")
st.title("ğŸ›¸ Drone Info Assistant â€” RAG ê¸°ë°˜ ë¬¸ì„œ ê²€ìƒ‰ & ë‹µë³€")

# Sidebar: engine options & admin
with st.sidebar:
    st.header("ì„¤ì •")
    use_ollama = st.checkbox("Use Ollama (ë¡œì»¬ LLM & Embeddings)", value=USE_OLLAMA_DEFAULT)
    temperature = st.slider("LLM ì˜¨ë„", min_value=0.0, max_value=1.0, value=0.2, step=0.05)
    k_retrieve = st.number_input("ê²€ìƒ‰í•  ë¬¸ì„œ ìˆ˜ (k)", value=4, min_value=1, max_value=10, step=1)
    stream_enabled = st.sidebar.checkbox("ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ", value=True)
    st.markdown("---")
    st.write("ì¸ë±ìŠ¤ ê´€ë¦¬")
    if st.button("ì¸ë±ìŠ¤ ë¡œë“œ"):
        st.session_state["vectorstore"] = load_or_build_index(INDEX_DIR)
        if st.session_state["vectorstore"]:
            st.success("âœ… ì¸ë±ìŠ¤ ë¡œë“œ ì™„ë£Œ")
        else:
            st.warning("ì¸ë±ìŠ¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. `build_vectorstore.py`ë¡œ ìƒì„±í•˜ì„¸ìš”.")
    if st.button("ì¸ë±ìŠ¤ ìƒíƒœ í™•ì¸"):
        vs = load_or_build_index(INDEX_DIR)
        if vs:
            st.write("Vectorstore info:", type(vs), getattr(vs, "index", "no-index"))
            st.success("ì¸ë±ìŠ¤ê°€ ì¡´ì¬í•©ë‹ˆë‹¤.")
        else:
            st.error("ì¸ë±ìŠ¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    rebuild = st.sidebar.button("ì¸ë±ìŠ¤ ì¬ë¹Œë“œ")
    if rebuild:
        import shutil
        shutil.rmtree(INDEX_DIR, ignore_errors=True)
        st.session_state["vectorstore"] = build_vectorstore()
        st.sidebar.success("ì¸ë±ìŠ¤ ì¬ìƒì„± ì™„ë£Œ")

# Load (or lazy load) vectorstore
if "vectorstore" not in st.session_state:
    st.session_state["vectorstore"] = load_or_build_index(INDEX_DIR)

if st.session_state["vectorstore"] is None:
    st.warning("FAISS ì¸ë±ìŠ¤ê°€ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì‚¬ì´ë“œë°”ì—ì„œ 'ì¸ë±ìŠ¤ ë¡œë“œ' ë¥¼ í´ë¦­í•˜ê±°ë‚˜ ìƒˆë¡œ ì¸ë±ìŠ¤ë¥¼ ë§Œë“œì„¸ìš”.")
    st.stop()

vectorstore: FAISS = st.session_state["vectorstore"]

# Category filter (derived from metadata if available)
all_categories = sorted(list({d.metadata.get("category","ê¸°íƒ€") for d in vectorstore.docstore._dict.values()})) if hasattr(vectorstore, "docstore") else ["ë²•ë¥ ","ë§¤ë‰´ì–¼","ê¸°íƒ€"]
selected_categories = st.multiselect("ê²€ìƒ‰í•  ì¹´í…Œê³ ë¦¬ í•„í„° (ë¹„ì›Œë‘ë©´ ì „ì²´ ê²€ìƒ‰)", options=all_categories, default=[])

# Chat history init
if "chat_history" not in st.session_state:
    st.session_state["chat_history"] = []

# Query input
st.markdown("### ì§ˆë¬¸ ì…ë ¥")
query = st.text_input("ë“œë¡  ê´€ë ¨ ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš” (ì˜ˆ: ë¹„í–‰ í—ˆê°€ ì ˆì°¨ëŠ” ì–´ë–»ê²Œ ë˜ë‚˜ìš”?)")
lang = detect_language(query)

col1, col2 = st.columns([1, 1])
with col1:
    if st.button("ğŸ” ê²€ìƒ‰ ë° ë‹µë³€", use_container_width=True):
        if not query.strip():
            st.warning("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”.")
        else:
            # Create retriever with possible filter
            retriever = vectorstore.as_retriever(search_kwargs={"k": k_retrieve})
            # If category filters present, construct simple metadata filter if vectorstore supports it
            if selected_categories:
                def filtered_retriever(q, k=k_retrieve):
                    docs = retriever.vectorstore.similarity_search(query, k=k_retrieve)
                    # client-side filter fallback: filter by metadata category and slice k
                    filtered = [d for d in docs if d.metadata.get("category") in selected_categories]
                    return filtered[:k]
                # use filtered_retriever for retrieval
                retrieved_docs = filtered_retriever(query)
            else:
                retrieved_docs = retriever.vectorstore.similarity_search(query, k=k_retrieve)
                if not retrieved_docs:
                    st.warning("âš ï¸ ë“œë¡  ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì§ˆë¬¸ì„ ë‹¤ì‹œ ì…ë ¥í•´ì£¼ì„¸ìš”.")
                    st.stop()

            # Display retrieved docs (collapsed)
            with st.expander(f"ğŸ” ê²€ìƒ‰ëœ {len(retrieved_docs)}ê°œ ë¬¸ì„œ ë³´ê¸°"):
                for d in retrieved_docs:
                    title = d.metadata.get("title", d.metadata.get("source", "unknown"))
                    chunk = d.metadata.get("chunk_id", "")
                    st.markdown(f"**{title}** (chunk: {chunk})")
                    st.write(d.page_content[:1000] + ("..." if len(d.page_content) > 1000 else ""))

            # Prepare LLM
            llm = get_llm_stream(stream_enabled)
            # Build prompt: include retrieved docs as context + explicit instruction to cite sources
            context_text = docs_to_display_text(retrieved_docs)
            system_template = (
                f"**ë°˜ë“œì‹œ** { 'í•œêµ­ì–´ë¡œ' if lang=='ko' else 'ì˜ì–´ë¡œ' } ë‹µë³€í•˜ì„¸ìš”.\n\n"
                "ë¬¸ì„œ ì™¸ë¶€ì˜ ì •ë³´ë‚˜ URL, ì¶œì²˜, ê·œì •, ìˆ˜ì¹˜ë¥¼ ìƒì„±í•˜ê±°ë‚˜ ì¶”ì¸¡í•˜ì§€ ë§ˆì„¸ìš”.\n\n"
                "ë‹¤ìŒì— ì œì‹œëœ ë¬¸ì„œë“¤ì„ **ìœ ì¼í•œ ì •ë³´ ì¶œì²˜**ë¡œ ì‚¬ìš©í•´ì•¼ í•©ë‹ˆë‹¤. "
                "ë‹¹ì‹ ì€ ë“œë¡  ê´€ë ¨ ë²•ê·œì™€ ê¸°ìˆ  ë¬¸ì„œì— ëŒ€í•œ ì „ë¬¸ ë¶„ì„ê°€ì…ë‹ˆë‹¤. "
                "ì—„ê²©í•œ ì§€ì¹¨:\n"
                "1. ì•„ë˜ ì°¸ê³  ë¬¸ì„œì˜ ë‚´ìš©ë§Œì„ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€í•˜ì„¸ìš”.\n"
                "2. ë¬¸ì„œì— í¬í•¨ë˜ì§€ ì•Šì€ URL, ê¸°ê´€ëª…, ìˆ˜ì¹˜, ê·œì •ì„ ì ˆëŒ€ë¡œ ìƒì„±í•˜ì§€ ë§ˆì„¸ìš”.\n"
                "3. ë¬¸ì„œì— 'url' ë˜ëŠ” 'source' ë©”íƒ€ë°ì´í„°ê°€ ì—†ëŠ” ê²½ìš°, ê·¸ í•­ëª©ì€ ìƒëµí•˜ì„¸ìš”.\n"
                "4. 'ì°¸ê³ ë¬¸í—Œ' ì„¹ì…˜ì—ëŠ” ì‹¤ì œ ë¬¸ì„œ ë©”íƒ€ë°ì´í„°ì— ì¡´ì¬í•˜ëŠ” title, urlë§Œ í‘œì‹œí•˜ì„¸ìš”.\n"
                "5. ë¬¸ì„œ ë‚´ìš©ì´ ë¶ˆì¶©ë¶„í•  ê²½ìš°, 'í•´ë‹¹ ë¬¸ì„œì—ì„œëŠ” ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.' ë¼ê³  ëª…ì‹œí•˜ì„¸ìš”.\n"
                "ì§ˆë¬¸ì´ ë“œë¡ ê³¼ ë¬´ê´€í•  ê²½ìš°, ë°˜ë“œì‹œ ë‹¤ìŒì²˜ëŸ¼ ë‹µí•˜ì„¸ìš”: ì´ ì‹œìŠ¤í…œì€ ë“œë¡  ê´€ë ¨ ì •ë³´ë§Œ ì œê³µí•©ë‹ˆë‹¤.\n"
                "=== ì°¸ê³  ë¬¸ì„œ ì‹œì‘ ===\n\n"
                "{context}\n"
                "=== ì°¸ê³  ë¬¸ì„œ ë ===\n\n"
            )
            user_template = f"({ 'í•œêµ­ì–´ë¡œ' if lang=='ko' else 'in English' }) ì‚¬ìš©ì ì§ˆë¬¸: {query}\n\nìš”ì•½í•˜ê³  ë‹¨ê³„ë³„ë¡œ ì„¤ëª…í•˜ì„¸ìš”."
            system_prompt = system_template.format(context=context_text)
            user_prompt = user_template.format(query=query)

            # Compose messages according to model interface
            messages = [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt}
            ]

            st.markdown("### âœ¨ ì‹¤ì‹œê°„ ë‹µë³€")
            response_area = st.empty()
            partial = ""
            try:
                with st.spinner("AIê°€ ë‹µë³€ ì¤‘ì…ë‹ˆë‹¤..."):
                    for chunk in llm.stream(messages):
                        token = getattr(chunk, "content", "")
                        partial += token
                        response_area.markdown(partial)
                answer_text = partial
            except Exception as e:
                st.error(f"LLM í˜¸ì¶œ ì‹¤íŒ¨: {e}")
                answer_text = f"ì˜¤ë¥˜ë¡œ ì¸í•´ ë‹µë³€ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {e}"

            # Save history
            st.session_state["chat_history"].insert(0, {"q": query, "a": answer_text})
            st.rerun()

with col2:
    if st.button("ğŸ§¾ ëŒ€í™” ê¸°ë¡ ì´ˆê¸°í™”", use_container_width=True):
        st.session_state["chat_history"] = []
        st.rerun()

# Show chat history
st.markdown("### ëŒ€í™” ê¸°ë¡")
if "chat_history" in st.session_state:
    for i, chat in enumerate(st.session_state["chat_history"], start=1):
        st.markdown(f"**{i}ï¸âƒ£ ì§ˆë¬¸:** {chat['q']}")
        st.markdown(f"ğŸ’¬ {chat['a']}")
        st.markdown("---")

# Optional: allow uploading new doc and indexing in-memory
st.markdown("### ë¬¸ì„œ ì—…ë¡œë“œ ë° ì¸ë±ìŠ¤ì— ì¶”ê°€ (ì„ íƒ)")
uploaded_files = st.file_uploader("PDF / í…ìŠ¤íŠ¸ íŒŒì¼ ì—…ë¡œë“œ (ì—¬ëŸ¬ê°œ ê°€ëŠ¥)", accept_multiple_files=True)
if uploaded_files:
    if st.button("â• ì—…ë¡œë“œ ë¬¸ì„œ ì¸ë±ì‹± (ë©”ëª¨ë¦¬)"):
        embeddings = get_embeddings(use_ollama=USE_OLLAMA_DEFAULT)
        new_docs = []
        import fitz, io
        for f in uploaded_files:
            filename = f.name
            if filename.lower().endswith(".pdf"):
                # extract text quickly via PyMuPDF
                pdf_bytes = f.read()
                pdf_doc = fitz.open(stream=pdf_bytes, filetype="pdf")
                txt = ""
                for p in pdf_doc:
                    txt += p.get_text("text") + "\n"
            else:
                txt = f.getvalue().decode("utf-8")
            # split naive (could use RecursiveCharacterTextSplitter if available)
            # We'll create a single doc per file for simplicity
            new_docs.append(Document(page_content=txt, metadata={"title": filename, "category": "ì—…ë¡œë“œ"}))

        try:
            # add_documents available in many FAISS wrappers
            vectorstore.add_documents(new_docs, embeddings=embeddings)
            st.success("ì—…ë¡œë“œ ë¬¸ì„œë¥¼ ì¸ë©”ëª¨ë¦¬ë¡œ ì¸ë±ì‹±í–ˆìŠµë‹ˆë‹¤. (ì•± ì¬ì‹œì‘ ì „ ìœ ì§€)")
        except Exception as e:
            st.error(f"ì¸ë±ìŠ¤ ì¶”ê°€ ì‹¤íŒ¨: {e}")

st.markdown("### ì‚¬ìš© íŒ")
st.write(
    "- ê²€ìƒ‰ ì‹œ ì›í•˜ëŠ” ì¹´í…Œê³ ë¦¬ë¥¼ ì„ íƒí•˜ë©´ ê´€ë ¨ ë¬¸ì„œë§Œ ìš°ì„  ê²€ìƒ‰í•©ë‹ˆë‹¤.\n"
    "- ë‹µë³€ì— ì¶œì²˜ê°€ ëª…ì‹œë˜ì§€ ì•Šìœ¼ë©´ í•´ë‹¹ ì§ˆë¬¸ì— ëŒ€í•œ ê·¼ê±° ë¬¸ì„œê°€ ë¶€ì¡±í•˜ë‹¤ëŠ” ëœ»ì…ë‹ˆë‹¤.\n"
)

