#!/usr/bin/env python3
"""
Streamlit RAG ì•± â€” Drone Info Assistant
- FAISS ì¸ë±ìŠ¤ì—ì„œ ë¬¸ë‹¨ì„ ê²€ìƒ‰í•˜ê³  LLMìœ¼ë¡œ ê·¼ê±° í¬í•¨ ë‹µë³€ ìƒì„±
- Ollama (ë¡œì»¬) ë˜ëŠ” OpenAI ì‚¬ìš© ê°€ëŠ¥
"""

import os
import json
import streamlit as st
from typing import List, Dict, Optional

from langchain_community.chat_models import ChatOllama
from langchain_community.embeddings import OllamaEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_core.documents import Document

# -------- Config --------
INDEX_DIR = "vector_index"        # FAISS ì €ì¥ ìœ„ì¹˜ (build_vectorstore.py ì—ì„œ ë§Œë“  í´ë”)
JSON_PATH = "drone_docs.json"     # ì›ë³¸ JSON (ì„ íƒì )
USE_OLLAMA_DEFAULT = True         # ê¸°ë³¸ LLM (ë³€ê²½ ê°€ëŠ¥)

# -------- Helpers --------
def load_vectorstore(index_dir: str) -> Optional[FAISS]:
    if not os.path.exists(index_dir):
        return None
    try:
        embeddings = OllamaEmbeddings(model="llama3.1:8b")
        vs = FAISS.load_local(index_dir, embeddings, allow_dangerous_deserialization=True)
        return vs
    except Exception as e:
        st.error(f"Vectorstore ë¡œë“œ ì‹¤íŒ¨: {e}")
        return None

def get_llm(use_ollama: bool, temperature: float = 0.2):
    """LLM ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜ (Ollama or OpenAI)"""
    if use_ollama:
        return ChatOllama(model="exaone3.5:7.8b", temperature=temperature)

def get_embeddings(use_ollama: bool):
    if use_ollama:
        return OllamaEmbeddings(model="llama3.1:8b")

def docs_to_display_text(docs: List[Document]) -> str:
    """ê²€ìƒ‰ëœ ë¬¸ì„œ(ë“¤)ë¥¼ ì½ê¸° ì¢‹ì€ ë¬¸ìì—´ë¡œ ë³€í™˜ (ì¶œì²˜ í¬í•¨)"""
    parts = []
    for d in docs:
        meta = d.metadata or {}
        title = meta.get("title", meta.get("source", "unknown"))
        chunk_id = meta.get("chunk_id", None)
        src = f"{title}" + (f" (chunk {chunk_id})" if chunk_id is not None else "")
        text = d.page_content if hasattr(d, "page_content") else str(d)
        parts.append(f"---\nğŸ“˜ {src}\n{text}")
    return "\n\n".join(parts)

# -------- Streamlit UI --------
st.set_page_config(page_title="ğŸ›¸ Drone Info Assistant (RAG)", layout="wide", page_icon="ğŸ›¸")
st.title("ğŸ›¸ Drone Info Assistant â€” RAG ê¸°ë°˜ ë¬¸ì„œ ê²€ìƒ‰ & ë‹µë³€")

# Sidebar: engine options & admin
with st.sidebar:
    st.header("ì„¤ì •")
    use_ollama = st.checkbox("Use Ollama (ë¡œì»¬ LLM & Embeddings)", value=USE_OLLAMA_DEFAULT)
    temperature = st.slider("LLM ì˜¨ë„", min_value=0.0, max_value=1.0, value=0.2, step=0.05)
    k_retrieve = st.number_input("ê²€ìƒ‰í•  ë¬¸ì„œ ìˆ˜ (k)", value=4, min_value=1, max_value=10, step=1)
    st.markdown("---")
    st.write("ì¸ë±ìŠ¤ ê´€ë¦¬")
    if st.button("ì¸ë±ìŠ¤ ë¡œë“œ"):
        st.session_state["vectorstore"] = load_vectorstore(INDEX_DIR)
        if st.session_state["vectorstore"]:
            st.success("âœ… ì¸ë±ìŠ¤ ë¡œë“œ ì™„ë£Œ")
        else:
            st.warning("ì¸ë±ìŠ¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. `build_vectorstore.py`ë¡œ ìƒì„±í•˜ì„¸ìš”.")
    if st.button("ì¸ë±ìŠ¤ ìƒíƒœ í™•ì¸"):
        vs = load_vectorstore(INDEX_DIR)
        if vs:
            st.write("Vectorstore info:", type(vs), getattr(vs, "index", "no-index"))
            st.success("ì¸ë±ìŠ¤ê°€ ì¡´ì¬í•©ë‹ˆë‹¤.")
        else:
            st.error("ì¸ë±ìŠ¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

# Load (or lazy load) vectorstore
if "vectorstore" not in st.session_state:
    st.session_state["vectorstore"] = load_vectorstore(INDEX_DIR)

if st.session_state["vectorstore"] is None:
    st.warning("FAISS ì¸ë±ìŠ¤ê°€ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì‚¬ì´ë“œë°”ì—ì„œ 'ì¸ë±ìŠ¤ ë¡œë“œ' ë¥¼ í´ë¦­í•˜ê±°ë‚˜ ë¨¼ì € build_vectorstore.py ë¡œ ì¸ë±ìŠ¤ë¥¼ ë§Œë“œì„¸ìš”.")
    st.stop()

vectorstore: FAISS = st.session_state["vectorstore"]

# Category filter (derived from metadata if available)
all_categories = sorted(list({d.metadata.get("category","ê¸°íƒ€") for d in vectorstore.docstore._dict.values()})) if hasattr(vectorstore, "docstore") else ["ë²•ë¥ ","ë§¤ë‰´ì–¼","ê¸°íƒ€"]
selected_categories = st.multiselect("ê²€ìƒ‰í•  ì¹´í…Œê³ ë¦¬ í•„í„° (ë¹„ì›Œë‘ë©´ ì „ì²´ ê²€ìƒ‰)", options=all_categories, default=[])

# Chat history init
if "chat_history" not in st.session_state:
    st.session_state["chat_history"] = []

# Query input
st.markdown("### ì§ˆë¬¸ ì…ë ¥")
query = st.text_input("ë“œë¡  ê´€ë ¨ ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš” (ì˜ˆ: ë¹„í–‰ í—ˆê°€ ì ˆì°¨ëŠ” ì–´ë–»ê²Œ ë˜ë‚˜ìš”?)")

col1, col2 = st.columns([1, 1])
with col1:
    if st.button("ğŸ” ê²€ìƒ‰ ë° ë‹µë³€", use_container_width=True):
        if not query.strip():
            st.warning("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”.")
        else:
            # Create retriever with possible filter
            retriever = vectorstore.as_retriever(search_kwargs={"k": k_retrieve})
            # If category filters present, construct simple metadata filter if vectorstore supports it
            if selected_categories:
                def filtered_retriever(q, k=k_retrieve):
                    docs = retriever.vectorstore.similarity_search(query, k=5)
                    # client-side filter fallback: filter by metadata category and slice k
                    filtered = [d for d in docs if d.metadata.get("category") in selected_categories]
                    return filtered[:k]
                # use filtered_retriever for retrieval
                retrieved_docs = filtered_retriever(query)
            else:
                retrieved_docs = retriever.vectorstore.similarity_search(query, k=5)
                if not retrieved_docs:
                    st.warning("âš ï¸ ë“œë¡  ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì§ˆë¬¸ì„ ë‹¤ì‹œ ì…ë ¥í•´ì£¼ì„¸ìš”.")
                    st.stop()

            # Display retrieved docs (collapsed)
            with st.expander(f"ğŸ” ê²€ìƒ‰ëœ {len(retrieved_docs)}ê°œ ë¬¸ì„œ ë³´ê¸°"):
                for d in retrieved_docs:
                    title = d.metadata.get("title", d.metadata.get("source", "unknown"))
                    chunk = d.metadata.get("chunk_id", "")
                    st.markdown(f"**{title}** (chunk: {chunk})")
                    st.write(d.page_content[:1000] + ("..." if len(d.page_content) > 1000 else ""))

            # Prepare LLM
            llm = get_llm(use_ollama=use_ollama, temperature=temperature)
            # Build prompt: include retrieved docs as context + explicit instruction to cite sources
            context_text = docs_to_display_text(retrieved_docs)
            system_template = (
                "ë‹¹ì‹ ì€ ë“œë¡  ê´€ë ¨ ë²•ê·œì™€ ê¸°ìˆ  ë¬¸ì„œì— ëŒ€í•œ ì „ë¬¸ ë¶„ì„ê°€ì…ë‹ˆë‹¤. "
                "ë‹¤ìŒì— ì œì‹œëœ ë¬¸ì„œë“¤ì„ **ìœ ì¼í•œ ì •ë³´ ì¶œì²˜**ë¡œ ì‚¬ìš©í•´ì•¼ í•©ë‹ˆë‹¤. "
                "ë¬¸ì„œ ì™¸ë¶€ì˜ ì •ë³´ë‚˜ URL, ì¶œì²˜, ê·œì •, ìˆ˜ì¹˜ë¥¼ ìƒì„±í•˜ê±°ë‚˜ ì¶”ì¸¡í•˜ì§€ ë§ˆì„¸ìš”.\n\n"
                "=== ì°¸ê³  ë¬¸ì„œ ì‹œì‘ ===\n"
                "{context}\n"
                "=== ì°¸ê³  ë¬¸ì„œ ë ===\n\n"
                "ì—„ê²©í•œ ì§€ì¹¨:\n"
                "1. ìœ„ ë¬¸ì„œì˜ ë‚´ìš©ë§Œì„ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€í•˜ì„¸ìš”.\n"
                "2. ë¬¸ì„œì— í¬í•¨ë˜ì§€ ì•Šì€ URL, ê¸°ê´€ëª…, ìˆ˜ì¹˜, ê·œì •ì„ ì ˆëŒ€ë¡œ ìƒì„±í•˜ì§€ ë§ˆì„¸ìš”.\n"
                "3. ë¬¸ì„œì— 'url' ë˜ëŠ” 'source' ë©”íƒ€ë°ì´í„°ê°€ ì—†ëŠ” ê²½ìš°, ê·¸ í•­ëª©ì€ ìƒëµí•˜ì„¸ìš”.\n"
                "4. 'ì°¸ê³ ë¬¸í—Œ' ì„¹ì…˜ì—ëŠ” ì‹¤ì œ ë¬¸ì„œ ë©”íƒ€ë°ì´í„°ì— ì¡´ì¬í•˜ëŠ” title, urlë§Œ í‘œì‹œí•˜ì„¸ìš”.\n"
                "5. ë¬¸ì„œ ë‚´ìš©ì´ ë¶ˆì¶©ë¶„í•  ê²½ìš°, 'í•´ë‹¹ ë¬¸ì„œì—ì„œëŠ” ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.' ë¼ê³  ëª…ì‹œí•˜ì„¸ìš”.\n"
                "ì§ˆë¬¸ì´ ë“œë¡ ê³¼ ë¬´ê´€í•  ê²½ìš°, ë°˜ë“œì‹œ ë‹¤ìŒì²˜ëŸ¼ ë‹µí•˜ì„¸ìš”: ì´ ì‹œìŠ¤í…œì€ ë“œë¡  ê´€ë ¨ ì •ë³´ë§Œ ì œê³µí•©ë‹ˆë‹¤.\n"
                "6. ì‚¬ìš©ìì˜ ì§ˆë¬¸ ì–¸ì–´ë¥¼ ê°ì§€í•˜ê³ , ë°˜ë“œì‹œ ë™ì¼í•œ ì–¸ì–´ë¡œ ë‹µë³€í•˜ì„¸ìš”."
            )
            user_template = "ì‚¬ìš©ì ì§ˆë¬¸: {query}\n\nìš”ì•½í•˜ê³  ë‹¨ê³„ë³„ë¡œ ì„¤ëª…í•˜ì„¸ìš”."
            system_prompt = system_template.format(context=context_text)
            user_prompt = user_template.format(query=query)

            # Compose messages according to model interface
            messages = [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt}
            ]

            try:
                resp = llm.invoke(messages) if hasattr(llm, "invoke") else llm(messages)
                answer_text = getattr(resp, "content", str(resp))
            except Exception as e:
                st.error(f"LLM í˜¸ì¶œ ì‹¤íŒ¨: {e}")
                answer_text = f"ì˜¤ë¥˜ë¡œ ì¸í•´ ë‹µë³€ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {e}"

            # Save history
            st.session_state["chat_history"].append({"q": query, "a": answer_text})
            st.rerun()

with col2:
    if st.button("ğŸ§¾ ëŒ€í™” ê¸°ë¡ ì´ˆê¸°í™”", use_container_width=True):
        st.session_state["chat_history"] = []
        st.rerun()

# Show chat history
st.markdown("### ëŒ€í™” ê¸°ë¡")
for i, turn in enumerate(reversed(st.session_state["chat_history"]), 1):
    st.markdown(f"**Q{i}.** {turn['q']}")
    st.markdown(f"**A{i}.**")
    st.write(turn["a"])
    st.markdown("---")

# Optional: allow uploading new doc and indexing in-memory
st.markdown("### ë¬¸ì„œ ì—…ë¡œë“œ ë° ì¸ë±ìŠ¤ì— ì¶”ê°€ (ì„ íƒ)")
uploaded_files = st.file_uploader("PDF / í…ìŠ¤íŠ¸ íŒŒì¼ ì—…ë¡œë“œ (ì—¬ëŸ¬ê°œ ê°€ëŠ¥)", accept_multiple_files=True)
if uploaded_files:
    if st.button("â• ì—…ë¡œë“œ ë¬¸ì„œ ì¸ë±ì‹± (ë©”ëª¨ë¦¬)"):
        embeddings = get_embeddings(use_ollama=use_ollama)
        new_docs = []
        import fitz, io
        for f in uploaded_files:
            filename = f.name
            if filename.lower().endswith(".pdf"):
                # extract text quickly via PyMuPDF
                pdf_bytes = f.read()
                pdf_doc = fitz.open(stream=pdf_bytes, filetype="pdf")
                txt = ""
                for p in pdf_doc:
                    txt += p.get_text("text") + "\n"
            else:
                txt = f.getvalue().decode("utf-8")
            # split naive (could use RecursiveCharacterTextSplitter if available)
            # We'll create a single doc per file for simplicity
            new_docs.append(Document(page_content=txt, metadata={"title": filename, "category": "ì—…ë¡œë“œ"}))

        try:
            # add_documents available in many FAISS wrappers
            vectorstore.add_documents(new_docs, embeddings=embeddings)
            st.success("ì—…ë¡œë“œ ë¬¸ì„œë¥¼ ì¸ë©”ëª¨ë¦¬ë¡œ ì¸ë±ì‹±í–ˆìŠµë‹ˆë‹¤. (ì•± ì¬ì‹œì‘ ì „ ìœ ì§€)")
        except Exception as e:
            st.error(f"ì¸ë±ìŠ¤ ì¶”ê°€ ì‹¤íŒ¨: {e}")

st.markdown("### ì‚¬ìš© íŒ")
st.write(
    "- ê²€ìƒ‰ ì‹œ ì›í•˜ëŠ” ì¹´í…Œê³ ë¦¬ë¥¼ ì„ íƒí•˜ë©´ ê´€ë ¨ ë¬¸ì„œë§Œ ìš°ì„  ê²€ìƒ‰í•©ë‹ˆë‹¤.\n"
    "- ë‹µë³€ì— ì¶œì²˜ê°€ ëª…ì‹œë˜ì§€ ì•Šìœ¼ë©´ í•´ë‹¹ ì§ˆë¬¸ì— ëŒ€í•œ ê·¼ê±° ë¬¸ì„œê°€ ë¶€ì¡±í•˜ë‹¤ëŠ” ëœ»ì…ë‹ˆë‹¤.\n"
)

