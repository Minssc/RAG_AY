#!/usr/bin/env python3
"""
ğŸš Drone Info Assistant â€” RAG ê¸°ë°˜ Streamlit ì•±
- í´ë” ë‚´ PDFë¥¼ ìë™ ë¡œë“œí•˜ì—¬ ë²¡í„° ì¸ë±ìŠ¤ êµ¬ì¶•
- Ollama ë˜ëŠ” OpenAI ê¸°ë°˜ LLM ì§€ì› ê°€ëŠ¥
"""

import os
import glob
from pathlib import Path
import faiss
import streamlit as st
from typing import List
from langchain_community.chat_models import ChatOllama
from langchain_community.embeddings import OllamaEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_community.docstore import InMemoryDocstore
from langchain_community.document_loaders import PyPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_core.documents import Document

# =====================================
# ğŸ”§ ê¸°ë³¸ ì„¤ì •
# =====================================
PDF_DIR = "drone_pdfs"      # PDF í´ë”ëª… (donre_pdfs â†’ ì˜¤íƒ€ ìˆ˜ì •)
TEXT_DIR = "data"
INDEX_DIR = "faiss_index"   # ì¸ë±ìŠ¤ ì €ì¥ í´ë”
USE_OLLAMA_DEFAULT = True   # ê¸°ë³¸ ì„¤ì •ê°’

st.set_page_config(page_title="ğŸ›¸ Drone Info Assistant", page_icon="ğŸ›¸", layout="wide")
st.title("ğŸ›¸ Drone Info Assistant â€” RAG ê¸°ë°˜ ë¬¸ì„œ ê²€ìƒ‰ & ë‹µë³€")


# =====================================
# ğŸ“„ DATA ë¡œë“œ
# =====================================
def load_all_data(pdf_dir: str, text_dir: str = "data"):
    text_paths = []
    for file_path in Path(text_dir).rglob('*'):
        if file_path.suffix.lower() not in ['.md', '.rst']:
            continue
        text_paths.append(file_path.as_posix())

    pdf_paths = glob.glob(os.path.join(pdf_dir, "*.pdf"))
    if not pdf_paths:
        st.error(f"âŒ PDF í´ë”({pdf_dir})ì— íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        return [] # skip text folder checking

    all_docs = []
    for pdf_path in pdf_paths:
        loader = PyPDFLoader(pdf_path)
        try:
            docs = loader.load()
            for d in docs:
                d.metadata["source"] = os.path.basename(pdf_path)
            all_docs.extend(docs)
            st.info(f"ğŸ“˜ {os.path.basename(pdf_path)} ë¡œë“œ ì™„ë£Œ ({len(docs)} í˜ì´ì§€)")
        except Exception as e:
            st.warning(f"âš ï¸ {pdf_path} ë¡œë“œ ì‹¤íŒ¨: {e}")

    for text_path in text_paths:
        with open(text_path, 'r') as f:
            content = f.read()
        all_docs.append(Document(
            page_content=content,
            metadata={
                "source": os.path.basename(text_path),
                "path": text_path,
                "type": "text"
            }           
        ))

    st.info(f"ğŸ“˜ data í´ë” ë‚´ md,rst íŒŒì¼ ë¡œë“œ ì™„ë£Œ: ({len(text_paths)}) íŒŒì¼")
        
    return all_docs


# =====================================
# ğŸ§  ë²¡í„°ìŠ¤í† ì–´ êµ¬ì¶•
# =====================================
@st.cache_resource(show_spinner=False)
def build_vectorstore():
    all_docs = load_all_data(PDF_DIR)
    if not all_docs:
        return None

    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
    splits = text_splitter.split_documents(all_docs)
    st.success(f"ì´ {len(splits)}ê°œ ë¬¸ì„œ ì²­í¬ ìƒì„± ì™„ë£Œ!")

    class InstructEmbeddings(OllamaEmbeddings):
        def embed_documents(self, texts: List[str]) -> List[List[float]]:
            instructed_texts = [f"Represent this sentence for retrieval: {t}" for t in texts]
            return super().embed_documents(instructed_texts)

        def embed_query(self, text: str) -> List[float]:
            instructed_text = f"Represent this sentence for retrieval: {text}"
            return super().embed_documents([instructed_text])[0]

    embeddings = InstructEmbeddings(model="llama3.1:8b")
    embedding_dim = len(embeddings.embed_query("í…ŒìŠ¤íŠ¸"))
    index = faiss.IndexFlatL2(embedding_dim)

    vectorstore = FAISS(
        embedding_function=embeddings,
        index=index,
        docstore=InMemoryDocstore(),
        index_to_docstore_id={},
    )

    batch_size = 100
    for i in range(0, len(splits), batch_size):
        vectorstore.add_documents(splits[i:i + batch_size])

    os.makedirs(INDEX_DIR, exist_ok=True)
    vectorstore.save_local(INDEX_DIR)
    st.success(f"âœ… FAISS ì¸ë±ìŠ¤ ìƒì„± ì™„ë£Œ ë° {INDEX_DIR}/ ì— ì €ì¥ë¨")
    return vectorstore


# =====================================
# ğŸ’¾ ì¸ë±ìŠ¤ ë¡œë“œ
# =====================================
def load_vectorstore(index_dir: str):
    if not os.path.exists(index_dir):
        return None

    embeddings = OllamaEmbeddings(model="llama3.1:8b")
    try:
        vs = FAISS.load_local(index_dir, embeddings, allow_dangerous_deserialization=True)
        return vs
    except Exception as e:
        st.error(f"ì¸ë±ìŠ¤ ë¡œë“œ ì‹¤íŒ¨: {e}")
        return None


# =====================================
# ğŸ§© LLM ìƒì„±ê¸° (Ollama / Exaone ì„ íƒ ê°€ëŠ¥)
# =====================================
def get_llm(use_ollama=True, temperature=0.3):
    if use_ollama:
        return ChatOllama(model="exaone3.5:7.8b", temperature=temperature)


# =====================================
# ğŸ“‹ ë„ìš°ë¯¸ í•¨ìˆ˜
# =====================================
def docs_to_display_text(docs: List[Document]) -> str:
    """ê²€ìƒ‰ëœ ë¬¸ì„œë“¤ì„ í”„ë¡¬í”„íŠ¸ìš© ë¬¸ìì—´ë¡œ ë³€í™˜"""
    parts = []
    for d in docs:
        title = d.metadata.get("title", d.metadata.get("source", "unknown"))
        chunk = d.metadata.get("chunk_id", "")
        parts.append(f"[{title} - chunk {chunk}]\n{d.page_content}")
    return "\n\n".join(parts)


# =====================================
# ğŸ§­ ì‚¬ì´ë“œë°” ì„¤ì •
# =====================================
with st.sidebar:
    st.header("âš™ï¸ ì„¤ì •")
    use_ollama = st.checkbox("Use Ollama (ë¡œì»¬ LLM)", value=USE_OLLAMA_DEFAULT)
    temperature = st.slider("LLM Temperature", 0.0, 1.0, 0.2, 0.05)
    k_retrieve = st.number_input("ê²€ìƒ‰í•  ë¬¸ì„œ ìˆ˜ (k)", 1, 10, 4)

    st.markdown("---")
    st.subheader("ğŸ“¦ ì¸ë±ìŠ¤ ê´€ë¦¬")

    if st.button("ì¸ë±ìŠ¤ ìƒˆë¡œ êµ¬ì¶•"):
        st.session_state["vectorstore"] = build_vectorstore()

    if st.button("ì¸ë±ìŠ¤ ë¡œë“œ"):
        st.session_state["vectorstore"] = load_vectorstore(INDEX_DIR)
        if st.session_state["vectorstore"]:
            st.success("âœ… ì¸ë±ìŠ¤ ë¡œë“œ ì™„ë£Œ")
        else:
            st.warning("ì¸ë±ìŠ¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

# =====================================
# ğŸ” ì§ˆì˜ì‘ë‹µ
# =====================================
if "vectorstore" not in st.session_state:
    st.session_state["vectorstore"] = load_vectorstore(INDEX_DIR)

vectorstore = st.session_state.get("vectorstore")
if vectorstore is None:
    st.warning("â— ì¸ë±ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤. ì‚¬ì´ë“œë°”ì—ì„œ ìƒˆë¡œ êµ¬ì¶•í•˜ê±°ë‚˜ ë¡œë“œí•˜ì„¸ìš”.")
    st.stop()

query = st.text_input("ë“œë¡  ê´€ë ¨ ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš” (ì˜ˆ: ë¹„í–‰ í—ˆê°€ ì ˆì°¨ëŠ” ì–´ë–»ê²Œ ë˜ë‚˜ìš”?)")

if st.button("ğŸ” ê²€ìƒ‰ ë° ë‹µë³€"):
    if not query.strip():
        st.warning("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”.")
        st.stop()

    retriever = vectorstore.as_retriever(search_kwargs={"k": k_retrieve})
    retrieved_docs = retriever.invoke(query)
    if not retrieved_docs:
        st.warning("ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        st.stop()

    with st.expander(f"ğŸ” ê²€ìƒ‰ëœ ë¬¸ì„œ ({len(retrieved_docs)}ê°œ)"):
        for d in retrieved_docs:
            st.markdown(f"**{d.metadata.get('source','unknown')}**")
            st.write(d.page_content[:500] + "...")

    context_text = docs_to_display_text(retrieved_docs)

    system_prompt = (
        "ë‹¹ì‹ ì€ ë“œë¡  ê´€ë ¨ ë²•ê·œ ë° ê¸°ìˆ  ë¬¸ì„œ ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. "
        "ì œê³µëœ ë¬¸ì„œ ì™¸ì˜ ì •ë³´ë‚˜ URLì„ ìƒì„±í•˜ì§€ ë§ˆì„¸ìš”.\n\n"
        f"=== ì°¸ê³  ë¬¸ì„œ ===\n{context_text}\n=== ë ==="
        "ì—„ê²©í•œ ì§€ì¹¨:\n"
        "1. ìœ„ ë¬¸ì„œì˜ ë‚´ìš©ë§Œì„ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€í•˜ì„¸ìš”.\n"
        "2. ë¬¸ì„œì— í¬í•¨ë˜ì§€ ì•Šì€ URL, ê¸°ê´€ëª…, ìˆ˜ì¹˜, ê·œì •ì„ ì ˆëŒ€ë¡œ ìƒì„±í•˜ì§€ ë§ˆì„¸ìš”.\n"
        "3. ë¬¸ì„œì— 'url' ë˜ëŠ” 'source' ë©”íƒ€ë°ì´í„°ê°€ ì—†ëŠ” ê²½ìš°, ê·¸ í•­ëª©ì€ ìƒëµí•˜ì„¸ìš”.\n"
        "4. 'ì°¸ê³ ë¬¸í—Œ' ì„¹ì…˜ì—ëŠ” ì‹¤ì œ ë¬¸ì„œ ë©”íƒ€ë°ì´í„°ì— ì¡´ì¬í•˜ëŠ” title, urlë§Œ í‘œì‹œí•˜ì„¸ìš”.\n"
        "5. ë¬¸ì„œ ë‚´ìš©ì´ ë¶ˆì¶©ë¶„í•  ê²½ìš°, 'í•´ë‹¹ ë¬¸ì„œì—ì„œëŠ” ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.' ë¼ê³  ëª…ì‹œí•˜ì„¸ìš”.\n"
        "ì§ˆë¬¸ì´ ë“œë¡ ê³¼ ë¬´ê´€í•  ê²½ìš°, ë°˜ë“œì‹œ ë‹¤ìŒì²˜ëŸ¼ ë‹µí•˜ì„¸ìš”: ì´ ì‹œìŠ¤í…œì€ ë“œë¡  ê´€ë ¨ ì •ë³´ë§Œ ì œê³µí•©ë‹ˆë‹¤.\n"
        "6. ì‚¬ìš©ìì˜ ì§ˆë¬¸ ì–¸ì–´ë¥¼ ê°ì§€í•˜ê³ , ë°˜ë“œì‹œ ë™ì¼í•œ ì–¸ì–´ë¡œ ë‹µë³€í•˜ì„¸ìš”."
    )

    user_prompt = f"ì§ˆë¬¸: {query}\n\në¬¸ì„œ ë‚´ìš©ë§Œ ê¸°ë°˜ìœ¼ë¡œ ì‹¤ë¬´ì  ë‹µë³€ì„ ì‘ì„±í•˜ì„¸ìš”."

    llm = get_llm(use_ollama=use_ollama, temperature=temperature)
    messages = [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": user_prompt},
    ]

    with st.spinner("AIê°€ ë‹µë³€ ì¤‘ì…ë‹ˆë‹¤..."):
        resp = llm.invoke(messages)
        answer = getattr(resp, "content", str(resp))

    st.markdown("## âœˆï¸ ë‹µë³€")
    st.write(answer)
