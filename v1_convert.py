#!/usr/bin/env python3
"""
ğŸ“˜ Drone PDF â†’ JSON ë³€í™˜ê¸°
ë“œë¡  ë²•ë¥ /ë§¤ë‰´ì–¼/ê°€ì´ë“œ ë¬¸ì„œë¥¼ RAGìš© JSON í¬ë§·ìœ¼ë¡œ ë³€í™˜
"""

import os
from pathlib import Path
import re
import json
import fitz  # PyMuPDF
from tqdm import tqdm
from langchain_text_splitters import RecursiveCharacterTextSplitter


def clean_text(text: str) -> str:
    """PDFì—ì„œ ì¶”ì¶œëœ í…ìŠ¤íŠ¸ë¥¼ ì •ì œ"""
    text = re.sub(r'\s+', ' ', text)
    text = text.replace("â€", "-").strip()
    return text


def classify_category(filename: str) -> str:
    """íŒŒì¼ëª… ê¸°ë°˜ ê°„ë‹¨í•œ ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜"""
    name = filename.lower()
    if "law" in name or "ë²•" in name:
        return "ë²•ë¥ "
    elif "manual" in name or "guide" in name or "controller" in name:
        return "ë§¤ë‰´ì–¼"
    else:
        return "ê¸°íƒ€"


def extract_text_from_pdf(pdf_path: str) -> str:
    """PDFì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ (PyMuPDF)"""
    doc = fitz.open(pdf_path)
    text = ""
    for page in doc:
        page_text = page.get_text("text")
        text += page_text + "\n"
    return clean_text(text)

def extract_text_from_file(md_path: str) -> str:
    """mdíŒŒì¼ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
    with open(md_path, "r") as f:
        text = f.read()
    return clean_text(text)


def split_into_chunks(text: str, chunk_size=1000, overlap=150):
    """LangChain TextSplitterë¡œ ë¬¸ë‹¨ ë¶„ë¦¬"""
    splitter = RecursiveCharacterTextSplitter(
        chunk_size=chunk_size,
        chunk_overlap=overlap,
        separators=["\n\n", "\n", ".", " "]
    )
    return splitter.split_text(text)


def convert_data_to_json(pdf_folder="drone_pdfs", text_folder='data', output_path="drone_docs.json"):
    """PDF ë° md,rstíŒŒì¼ ì „ì²´ ë³€í™˜"""
    all_docs = []

    pdf_files = [f for f in os.listdir(pdf_folder) if f.lower().endswith(".pdf")]
    text_files = []
    for file_path in Path(text_folder).rglob('*'):
        if file_path.suffix.lower() not in ['.md', '.rst']:
            continue
        text_files.append(file_path.as_posix())

    if not pdf_files:
        print("âš ï¸ PDF íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        return

    for filename in tqdm(pdf_files, desc="PDF ë³€í™˜ ì¤‘"):
        pdf_path = os.path.join(pdf_folder, filename)
        category = classify_category(filename)
        text = extract_text_from_pdf(pdf_path)

        chunks = split_into_chunks(text)

        for idx, chunk in enumerate(chunks):
            all_docs.append({
                "title": filename.replace(".pdf", ""),
                "category": category,
                "chunk_id": idx,
                "content": chunk,
                "source": pdf_path
            })

    for file_path in tqdm(text_files, desc="md, rst ë³€í™˜ ì¤‘"):
        category = "ë§¤ë‰´ì–¼" ### 
        text = extract_text_from_file(file_path)

        chunks = split_into_chunks(text)

        for idx, chunk in enumerate(chunks):
            all_docs.append({
                "title": filename.replace(".md", ""),
                "category": category,
                "chunk_id": idx,
                "content": chunk,
                "source": file_path
            })

    with open(output_path, "w", encoding="utf-8") as f:
        json.dump({"documents": all_docs}, f, ensure_ascii=False, indent=2)

    print(f"\nâœ… ë³€í™˜ ì™„ë£Œ: {output_path}")
    print(f"ì´ {len(all_docs)}ê°œì˜ ë¬¸ë‹¨ì´ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")

if __name__ == "__main__":
    os.makedirs("drone_pdfs", exist_ok=True)
    convert_data_to_json("drone_pdfs", 'data', "drone_docs.json")


# #!/usr/bin/env python3
# """
# ğŸ“˜ PDF â†’ JSON ë³€í™˜ê¸° (ì˜ë¯¸ ê¸°ë°˜ Chunk ë¶„í• )
# ë“œë¡  ê´€ë ¨ ë²•ë¥  ë° ê¸°ìˆ  ë§¤ë‰´ì–¼ ë¬¸ì„œë¥¼ RAG ì…ë ¥ìš© JSONìœ¼ë¡œ ë³€í™˜
# """

# import os
# import json
# import re
# import fitz  # PyMuPDF
# from typing import List, Dict

# # =============================
# # ğŸ”§ CONFIG
# # =============================
# PDF_DIR = "drone_pdfs"           # ë³€í™˜í•  PDF í´ë”
# OUTPUT_JSON = "drone_docs.json"
# CHUNK_SIZE = 1200          # chunk ìµœëŒ€ ê¸¸ì´
# OVERLAP = 150              # ë¬¸ë§¥ overlap (ë¬¸ì¥ ë‹¨ìœ„)
# MIN_TEXT_LEN = 100         # ë„ˆë¬´ ì§§ì€ í˜ì´ì§€ í•„í„°ë§

# # =============================
# # ğŸ§© HELPER FUNCTIONS
# # =============================
# def extract_text_from_pdf(path: str) -> str:
#     """PDF íŒŒì¼ì—ì„œ í…ìŠ¤íŠ¸ ì „ì²´ ì¶”ì¶œ"""
#     doc = fitz.open(path)
#     text = ""
#     for page in doc:
#         text += page.get_text("text") + "\n"
#     doc.close()
#     return text


# def smart_chunk_text(text: str, max_len: int = 1200, overlap: int = 150) -> List[str]:
#     """
#     ì˜ë¯¸ ê¸°ë°˜ í…ìŠ¤íŠ¸ ì²­í¬ ë¶„í• 
#     - í—¤ë”©(â€˜ì œ1ì¡°â€™, ìˆ«ì. , # ì œëª©) ê¸°ì¤€ 1ì°¨ ë¶„ë¦¬
#     - ë„ˆë¬´ ê¸¸ë©´ ë¬¸ì¥ ë‹¨ìœ„ë¡œ 2ì°¨ ë¶„ë¦¬
#     """
#     # 1ï¸âƒ£ í—¤ë”©/ì„¹ì…˜ ë‹¨ìœ„ë¡œ 1ì°¨ ë¶„í• 
#     sections = re.split(r'\n(?=(ì œ\s?\d+\s?ì¡°|[0-9]+\.\s|#{1,3}\s))', text)
#     chunks = []

#     for section in sections:
#         section = section.strip()
#         if len(section) < MIN_TEXT_LEN:
#             continue

#         # 2ï¸âƒ£ ê¸´ ì„¹ì…˜ì€ ë¬¸ì¥ ë‹¨ìœ„ë¡œ ë¶„ë¦¬
#         while len(section) > max_len:
#             split_idx = section[:max_len].rfind(".")
#             if split_idx == -1:
#                 split_idx = max_len
#             chunks.append(section[:split_idx])
#             section = section[split_idx - overlap:]
#         chunks.append(section)
#     return chunks


# # =============================
# # ğŸ§  MAIN CONVERSION PIPELINE
# # =============================
# def convert_pdfs_to_json(pdf_dir: str, output_path: str):
#     docs = []
#     total_chunks = 0

#     pdf_files = [f for f in os.listdir(pdf_dir) if f.lower().endswith(".pdf")]
#     for fname in pdf_files:
#         pdf_path = os.path.join(pdf_dir, fname)
#         print(f"ğŸ“„ ë³€í™˜ ì¤‘: {fname}")

#         try:
#             # âœ… í˜ì´ì§€ ë‹¨ìœ„ë¡œ ì²˜ë¦¬ (ìŠ¤íŠ¸ë¦¬ë° ë°©ì‹)
#             doc = fitz.open(pdf_path)
#             for page_num, page in enumerate(doc):
#                 text = page.get_text("text")
#                 if len(text.strip()) < MIN_TEXT_LEN:
#                     continue
#                 chunks = smart_chunk_text(text, max_len=CHUNK_SIZE, overlap=OVERLAP)
#                 for i, chunk in enumerate(chunks):
#                     docs.append({
#                         "source": fname,
#                         "page": page_num + 1,
#                         "chunk_id": f"{page_num}-{i}",
#                         "content": chunk,
#                         "title": infer_title(chunk),
#                     })
#                 total_chunks += len(chunks)
#             doc.close()

#             # âœ… ì¤‘ê°„ ì €ì¥ (10ê°œ íŒŒì¼ë§ˆë‹¤ flush)
#             if len(docs) > 1000:
#                 with open(output_path, "w", encoding="utf-8") as f:
#                     json.dump(docs, f, ensure_ascii=False, indent=2)
#                 docs = []  # ë©”ëª¨ë¦¬ ë¹„ì›€
#                 print("ğŸ’¾ ì¤‘ê°„ ì €ì¥ ì™„ë£Œ. ë©”ëª¨ë¦¬ ì •ë¦¬.")

#         except Exception as e:
#             print(f"âš ï¸ {fname} ë³€í™˜ ì‹¤íŒ¨: {e}")

#     # ë§ˆì§€ë§‰ flush
#     with open(output_path, "w", encoding="utf-8") as f:
#         json.dump(docs, f, ensure_ascii=False, indent=2)

#     print(f"\nâœ… ë³€í™˜ ì™„ë£Œ ({total_chunks}ê°œ ì²­í¬ ìƒì„±)")

# def infer_title(chunk: str) -> str:
#     """
#     Chunkì˜ ì²« ë¶€ë¶„ì—ì„œ ì œëª© ë˜ëŠ” ì„¹ì…˜ëª… ì¶”ì •
#     ex) 'ì œ3ì¡° ë“œë¡  ìš´í•­ ì œí•œ' â†’ 'ë“œë¡  ìš´í•­ ì œí•œ'
#     """
#     lines = chunk.strip().split("\n")
#     first_line = lines[0][:100] if lines else ""
#     title = re.sub(r"^(ì œ\s?\d+\s?ì¡°|[0-9]+\.\s|#\s*)", "", first_line).strip()
#     return title if title else "Untitled Section"


# # =============================
# # ğŸš€ ENTRY POINT
# # =============================
# if __name__ == "__main__":
#     convert_pdfs_to_json(PDF_DIR, OUTPUT_JSON)
